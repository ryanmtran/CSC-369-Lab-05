{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSC 369 - PySpark Introduction - Lab 5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samayNathani/CSC-369-Lab-05/blob/main/CSC_369_PySpark_Introduction_Lab_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yQ9QCIhsShu"
      },
      "source": [
        "# Introduction to Apache Spark / PySpark\n",
        "\n",
        "As of May 2021, these examples are based on [Spark 3.1.1](https://spark.apache.org/docs/3.1.1/)  \n",
        "\n",
        "Reference/API Links\n",
        "\n",
        "\n",
        "*   [Apache Spark Quick Start](https://spark.apache.org/docs/3.1.1/quick-start.html)\n",
        "*   [PySpark v3.1.1 API](https://spark.apache.org/docs/3.1.1/api/python/reference/index.html)\n",
        "*    [RDD Programming Guide](https://spark.apache.org/docs/3.1.1/rdd-programming-guide.html)\n",
        "*    [Spark SQL Programming Guide](https://spark.apache.org/docs/3.1.1/sql-programming-guide.html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoolY63aauhH",
        "outputId": "56facd94-d215-4ca7-fe1a-7c7aa4cbdf89"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n",
            "openjdk-8-jdk-headless is already the newest version (8u292-b10-0ubuntu1~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tad7fmwTbMwW"
      },
      "source": [
        "\n",
        "# Imports / Starter Example\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKUgYFX9olK5"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "from pyspark.sql import types as sparktypes\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "sc = SparkContext.getOrCreate() \n",
        "spark = SparkSession(sc)"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z53FFQEDayTj",
        "outputId": "ca28fce2-5035-4874-a13b-304685ac0c20"
      },
      "source": [
        "# create a Resilient Distributed Dataset (RDD) from a sequence of integers perform filter() and reduce() operations\n",
        "\n",
        "# Function to be used in the filter() transformation\n",
        "def filterSmall(x):    \n",
        "   if x < 20 == 0:\n",
        "      return False\n",
        "   else:\n",
        "      return True\n",
        "\n",
        "# Function to be used in the map() transformation\n",
        "def mapSquare(x):\n",
        "    return x*x\n",
        "\n",
        "# Function to be used in the reduce() action\n",
        "def reduceSim(x,y):\n",
        "    return x+y\n",
        "  \n",
        "rdd = sc.parallelize(range(100))         ## create an RDD of 100 numbers from 0 to 99\n",
        "\n",
        "out1 = rdd.filter(filterSmall).map(mapSquare)  ## perform filter and map transformations\n",
        "out2 = out1.reduce(reduceSim)                  ## perform reduce operation\n",
        "\n",
        "print(out1.collect())           ## print first output (all numbers less than 20 squared)\n",
        "print(out2)                 ## print second output (sum of all numbers from first output)\n"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529, 576, 625, 676, 729, 784, 841, 900, 961, 1024, 1089, 1156, 1225, 1296, 1369, 1444, 1521, 1600, 1681, 1764, 1849, 1936, 2025, 2116, 2209, 2304, 2401, 2500, 2601, 2704, 2809, 2916, 3025, 3136, 3249, 3364, 3481, 3600, 3721, 3844, 3969, 4096, 4225, 4356, 4489, 4624, 4761, 4900, 5041, 5184, 5329, 5476, 5625, 5776, 5929, 6084, 6241, 6400, 6561, 6724, 6889, 7056, 7225, 7396, 7569, 7744, 7921, 8100, 8281, 8464, 8649, 8836, 9025, 9216, 9409, 9604, 9801]\n",
            "328350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slRw9-jLsHOd"
      },
      "source": [
        "# download a sample access log for use in demos below\n",
        "# !rm -f apache.access.log\n",
        "# !wget -q https://raw.githubusercontent.com/databricks/reference-apps/master/logs_analyzer/data/apache.access.log"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmAntGt8iNli"
      },
      "source": [
        "# Apache HTTP Log Example - Resilient Distributed Dataset (RDD)\n",
        "\n",
        "A SparkContext instance can be used to create RDDs from various data/files/resources (text files, CSV, Hadoop data files, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUbUL01ff2j5",
        "outputId": "a276bd7f-0bd0-4ef9-c8e4-776e02d7e595"
      },
      "source": [
        "# find the top 10 clients, map/reduce style using RDD transformations\n",
        "access_log_rdd = (sc.textFile(\"apache.access.log\")\n",
        "                  .map(lambda line: ( line.split(\" \")[0], 1 ))  # field 0 = client address\n",
        "                  .reduceByKey(lambda x, y: x + y)\n",
        "                  .sortBy(lambda t: -t[1])) \n",
        "\n",
        "print (\"Total count of client hostnames:\")\n",
        "print(access_log_rdd.count())\n",
        "\n",
        "print (\"Top 10 client hostnames:\")\n",
        "print(access_log_rdd.take(10))\n"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total count of client hostnames:\n",
            "169\n",
            "Top 10 client hostnames:\n",
            "[('64.242.88.10', 452), ('10.0.0.153', 188), ('cr020r01-3.sac.overture.com', 44), ('h24-71-236-129.ca.shawcable.net', 36), ('h24-70-69-74.ca.shawcable.net', 32), ('market-mail.panduit.com', 29), ('ts04-ip92.hevanet.com', 28), ('ip68-228-43-49.tc.ph.cox.net', 22), ('proxy0.haifa.ac.il', 19), ('207.195.59.160', 15)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ca1vYvspy6_"
      },
      "source": [
        "# Apache HTTP Log Example - DataFrame\n",
        "\n",
        "A DataFrame is equivalent to a relational table in Spark SQL, and can be created from on a variety of input formats (CSV, JSON, relational database, etc.) using the SparkSession."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wccwGjPmBBL",
        "outputId": "7f74d7b6-9efd-4eb5-91c5-567b2c3a88c0"
      },
      "source": [
        "access_log_df = spark.read.text(\"apache.access.log\")\n",
        "\n",
        "access_log_df.show(truncate=False)\n",
        "access_log_df.printSchema()"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|value                                                                                                                                                      |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|64.242.88.10 - - [07/Mar/2004:16:05:49 -0800] \"GET /twiki/bin/edit/Main/Double_bounce_sender?topicparent=Main.ConfigurationVariables HTTP/1.1\" 401 12846   |\n",
            "|64.242.88.10 - - [07/Mar/2004:16:06:51 -0800] \"GET /twiki/bin/rdiff/TWiki/NewUserTemplate?rev1=1.3&rev2=1.2 HTTP/1.1\" 200 4523                             |\n",
            "|64.242.88.10 - - [07/Mar/2004:16:10:02 -0800] \"GET /mailman/listinfo/hsdivision HTTP/1.1\" 200 6291                                                         |\n",
            "|64.242.88.10 - - [07/Mar/2004:16:11:58 -0800] \"GET /twiki/bin/view/TWiki/WikiSyntax HTTP/1.1\" 200 7352                                                     |\n",
            "|64.242.88.10 - - [07/Mar/2004:16:20:55 -0800] \"GET /twiki/bin/view/Main/DCCAndPostFix HTTP/1.1\" 200 5253                                                   |\n",
            "|64.242.88.10 - - [07/Jun/2016:16:23:12 -0800] \"GET /twiki/bin/oops/TWiki/AppendixFileSystem?template=oopsmore&param1=1.12&param2=1.12 HTTP/1.1\" 200 11382  |\n",
            "|64.242.88.10 - - [07/Mar/2004:16:24:16 -0800] \"GET /twiki/bin/view/Main/PeterThoeny HTTP/1.1\" 200 4924                                                     |\n",
            "|64.242.88.10 - - [07/Mar/2004:16:29:16 -0800] \"GET /twiki/bin/edit/Main/Header_checks?topicparent=Main.ConfigurationVariables HTTP/1.1\" 401 12851          |\n",
            "|64.242.88.10 - - [07/Mar/2004:16:30:29 -0800] \"GET /twiki/bin/attach/Main/OfficeLocations HTTP/1.1\" 401 12851                                              |\n",
            "|64.242.88.10 - - [07/Mar/2004:16:31:48 -0800] \"GET /twiki/bin/view/TWiki/WebTopicEditTemplate HTTP/1.1\" 200 3732                                           |\n",
            "|64.242.88.10 - - [07/Mar/2004:16:32:50 -0800] \"GET /twiki/bin/view/Main/WebChanges HTTP/1.1\" 200 40520                                                     |\n",
            "|64.242.88.10 - - [07/Mar/2004:16:33:53 -0800] \"GET /twiki/bin/edit/Main/Smtpd_etrn_restrictions?topicparent=Main.ConfigurationVariables HTTP/1.1\" 401 12851|\n",
            "|64.242.88.10 - - [07/Mar/2004:16:35:19 -0800] \"GET /mailman/listinfo/business HTTP/1.1\" 200 6379                                                           |\n",
            "|64.242.88.10 - - [07/Mar/2004:16:36:22 -0800] \"GET /twiki/bin/rdiff/Main/WebIndex?rev1=1.2&rev2=1.1 HTTP/1.1\" 200 46373                                    |\n",
            "|64.242.88.10 - - [07/Jan/2004:16:37:27 -0800] \"GET /twiki/bin/view/TWiki/DontNotify HTTP/1.1\" 200 4140                                                     |\n",
            "|64.242.88.10 - - [07/Mar/2004:16:39:24 -0800] \"GET /twiki/bin/view/Main/TokyoOffice HTTP/1.1\" 200 3853                                                     |\n",
            "|64.242.88.10 - - [07/Feb/2003:16:43:54 -0800] \"GET /twiki/bin/view/Main/MikeMannix HTTP/1.1\" 200 3686                                                      |\n",
            "|64.242.88.10 - - [07/Mar/2004:16:45:56 -0800] \"GET /twiki/bin/attach/Main/PostfixCommands HTTP/1.1\" 401 12846                                              |\n",
            "|64.242.88.10 - - [07/Mar/2004:16:47:12 -0800] \"GET /robots.txt HTTP/1.1\" 200 68                                                                            |\n",
            "|64.242.88.10 - - [07/Mar/2004:16:47:46 -0800] \"GET /twiki/bin/rdiff/Know/ReadmeFirst?rev1=1.5&rev2=1.4 HTTP/1.1\" 200 5724                                  |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- value: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqc5g2nNqq3e",
        "outputId": "15900460-51af-4ab6-d936-cce4ce4bd9bb"
      },
      "source": [
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "\n",
        "access_log_df.show(truncate=False)\n",
        "access_log_df.printSchema()"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+---+---+---------------------+------+-------------------------------------------------------------------------------------------------+---+-----+\n",
            "|_c0         |_c1|_c2|_c3                  |_c4   |_c5                                                                                              |_c6|_c7  |\n",
            "+------------+---+---+---------------------+------+-------------------------------------------------------------------------------------------------+---+-----+\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:05:49|-0800]|GET /twiki/bin/edit/Main/Double_bounce_sender?topicparent=Main.ConfigurationVariables HTTP/1.1   |401|12846|\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:06:51|-0800]|GET /twiki/bin/rdiff/TWiki/NewUserTemplate?rev1=1.3&rev2=1.2 HTTP/1.1                            |200|4523 |\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:10:02|-0800]|GET /mailman/listinfo/hsdivision HTTP/1.1                                                        |200|6291 |\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:11:58|-0800]|GET /twiki/bin/view/TWiki/WikiSyntax HTTP/1.1                                                    |200|7352 |\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:20:55|-0800]|GET /twiki/bin/view/Main/DCCAndPostFix HTTP/1.1                                                  |200|5253 |\n",
            "|64.242.88.10|-  |-  |[07/Jun/2016:16:23:12|-0800]|GET /twiki/bin/oops/TWiki/AppendixFileSystem?template=oopsmore&param1=1.12&param2=1.12 HTTP/1.1  |200|11382|\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:24:16|-0800]|GET /twiki/bin/view/Main/PeterThoeny HTTP/1.1                                                    |200|4924 |\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:29:16|-0800]|GET /twiki/bin/edit/Main/Header_checks?topicparent=Main.ConfigurationVariables HTTP/1.1          |401|12851|\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:30:29|-0800]|GET /twiki/bin/attach/Main/OfficeLocations HTTP/1.1                                              |401|12851|\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:31:48|-0800]|GET /twiki/bin/view/TWiki/WebTopicEditTemplate HTTP/1.1                                          |200|3732 |\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:32:50|-0800]|GET /twiki/bin/view/Main/WebChanges HTTP/1.1                                                     |200|40520|\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:33:53|-0800]|GET /twiki/bin/edit/Main/Smtpd_etrn_restrictions?topicparent=Main.ConfigurationVariables HTTP/1.1|401|12851|\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:35:19|-0800]|GET /mailman/listinfo/business HTTP/1.1                                                          |200|6379 |\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:36:22|-0800]|GET /twiki/bin/rdiff/Main/WebIndex?rev1=1.2&rev2=1.1 HTTP/1.1                                    |200|46373|\n",
            "|64.242.88.10|-  |-  |[07/Jan/2004:16:37:27|-0800]|GET /twiki/bin/view/TWiki/DontNotify HTTP/1.1                                                    |200|4140 |\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:39:24|-0800]|GET /twiki/bin/view/Main/TokyoOffice HTTP/1.1                                                    |200|3853 |\n",
            "|64.242.88.10|-  |-  |[07/Feb/2003:16:43:54|-0800]|GET /twiki/bin/view/Main/MikeMannix HTTP/1.1                                                     |200|3686 |\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:45:56|-0800]|GET /twiki/bin/attach/Main/PostfixCommands HTTP/1.1                                              |401|12846|\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:47:12|-0800]|GET /robots.txt HTTP/1.1                                                                         |200|68   |\n",
            "|64.242.88.10|-  |-  |[07/Mar/2004:16:47:46|-0800]|GET /twiki/bin/rdiff/Know/ReadmeFirst?rev1=1.5&rev2=1.4 HTTP/1.1                                 |200|5724 |\n",
            "+------------+---+---+---------------------+------+-------------------------------------------------------------------------------------------------+---+-----+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            " |-- _c4: string (nullable = true)\n",
            " |-- _c5: string (nullable = true)\n",
            " |-- _c6: string (nullable = true)\n",
            " |-- _c7: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuD2cLn6p_Ob",
        "outputId": "17880867-6d85-4b5f-c3cc-36ab41da6f65"
      },
      "source": [
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "\n",
        "named_df = access_log_df.select(col('_c0').alias('host'),\n",
        "                                col('_c3').alias('timestamp'),\n",
        "                                col('_c5').alias('path'),\n",
        "                                col('_c6').cast('integer').alias('status'),\n",
        "                                col('_c7').cast('integer').alias('content_size'))\n",
        "named_df.show(truncate=False)\n",
        "named_df.printSchema()"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+---------------------+-------------------------------------------------------------------------------------------------+------+------------+\n",
            "|host        |timestamp            |path                                                                                             |status|content_size|\n",
            "+------------+---------------------+-------------------------------------------------------------------------------------------------+------+------------+\n",
            "|64.242.88.10|[07/Mar/2004:16:05:49|GET /twiki/bin/edit/Main/Double_bounce_sender?topicparent=Main.ConfigurationVariables HTTP/1.1   |401   |12846       |\n",
            "|64.242.88.10|[07/Mar/2004:16:06:51|GET /twiki/bin/rdiff/TWiki/NewUserTemplate?rev1=1.3&rev2=1.2 HTTP/1.1                            |200   |4523        |\n",
            "|64.242.88.10|[07/Mar/2004:16:10:02|GET /mailman/listinfo/hsdivision HTTP/1.1                                                        |200   |6291        |\n",
            "|64.242.88.10|[07/Mar/2004:16:11:58|GET /twiki/bin/view/TWiki/WikiSyntax HTTP/1.1                                                    |200   |7352        |\n",
            "|64.242.88.10|[07/Mar/2004:16:20:55|GET /twiki/bin/view/Main/DCCAndPostFix HTTP/1.1                                                  |200   |5253        |\n",
            "|64.242.88.10|[07/Jun/2016:16:23:12|GET /twiki/bin/oops/TWiki/AppendixFileSystem?template=oopsmore&param1=1.12&param2=1.12 HTTP/1.1  |200   |11382       |\n",
            "|64.242.88.10|[07/Mar/2004:16:24:16|GET /twiki/bin/view/Main/PeterThoeny HTTP/1.1                                                    |200   |4924        |\n",
            "|64.242.88.10|[07/Mar/2004:16:29:16|GET /twiki/bin/edit/Main/Header_checks?topicparent=Main.ConfigurationVariables HTTP/1.1          |401   |12851       |\n",
            "|64.242.88.10|[07/Mar/2004:16:30:29|GET /twiki/bin/attach/Main/OfficeLocations HTTP/1.1                                              |401   |12851       |\n",
            "|64.242.88.10|[07/Mar/2004:16:31:48|GET /twiki/bin/view/TWiki/WebTopicEditTemplate HTTP/1.1                                          |200   |3732        |\n",
            "|64.242.88.10|[07/Mar/2004:16:32:50|GET /twiki/bin/view/Main/WebChanges HTTP/1.1                                                     |200   |40520       |\n",
            "|64.242.88.10|[07/Mar/2004:16:33:53|GET /twiki/bin/edit/Main/Smtpd_etrn_restrictions?topicparent=Main.ConfigurationVariables HTTP/1.1|401   |12851       |\n",
            "|64.242.88.10|[07/Mar/2004:16:35:19|GET /mailman/listinfo/business HTTP/1.1                                                          |200   |6379        |\n",
            "|64.242.88.10|[07/Mar/2004:16:36:22|GET /twiki/bin/rdiff/Main/WebIndex?rev1=1.2&rev2=1.1 HTTP/1.1                                    |200   |46373       |\n",
            "|64.242.88.10|[07/Jan/2004:16:37:27|GET /twiki/bin/view/TWiki/DontNotify HTTP/1.1                                                    |200   |4140        |\n",
            "|64.242.88.10|[07/Mar/2004:16:39:24|GET /twiki/bin/view/Main/TokyoOffice HTTP/1.1                                                    |200   |3853        |\n",
            "|64.242.88.10|[07/Feb/2003:16:43:54|GET /twiki/bin/view/Main/MikeMannix HTTP/1.1                                                     |200   |3686        |\n",
            "|64.242.88.10|[07/Mar/2004:16:45:56|GET /twiki/bin/attach/Main/PostfixCommands HTTP/1.1                                              |401   |12846       |\n",
            "|64.242.88.10|[07/Mar/2004:16:47:12|GET /robots.txt HTTP/1.1                                                                         |200   |68          |\n",
            "|64.242.88.10|[07/Mar/2004:16:47:46|GET /twiki/bin/rdiff/Know/ReadmeFirst?rev1=1.5&rev2=1.4 HTTP/1.1                                 |200   |5724        |\n",
            "+------------+---------------------+-------------------------------------------------------------------------------------------------+------+------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- host: string (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- path: string (nullable = true)\n",
            " |-- status: integer (nullable = true)\n",
            " |-- content_size: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdpYRf3z9y_h",
        "outputId": "1d7782e5-8ae4-4805-b214-3e5578a3c960"
      },
      "source": [
        "named_df.createOrReplaceTempView(\"log\")\n",
        "sql_df = spark.sql(\"EXPLAIN FORMATTED SELECT * FROM log WHERE status = 404\")\n",
        "\n",
        "sql_df.show(truncate=False)\n",
        "sql_df.printSchema()"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|plan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|== Physical Plan ==\n",
            "* Project (3)\n",
            "+- * Filter (2)\n",
            "   +- Scan csv  (1)\n",
            "\n",
            "\n",
            "(1) Scan csv \n",
            "Output [5]: [_c0#7123, _c3#7126, _c5#7128, _c6#7129, _c7#7130]\n",
            "Batched: false\n",
            "Location: InMemoryFileIndex [file:/content/apache.access.log]\n",
            "PushedFilters: [IsNotNull(_c6)]\n",
            "ReadSchema: struct<_c0:string,_c3:string,_c5:string,_c6:string,_c7:string>\n",
            "\n",
            "(2) Filter [codegen id : 1]\n",
            "Input [5]: [_c0#7123, _c3#7126, _c5#7128, _c6#7129, _c7#7130]\n",
            "Condition : (isnotnull(_c6#7129) AND (cast(_c6#7129 as int) = 404))\n",
            "\n",
            "(3) Project [codegen id : 1]\n",
            "Output [5]: [_c0#7123 AS host#7139, _c3#7126 AS timestamp#7140, _c5#7128 AS path#7141, cast(_c6#7129 as int) AS status#7142, cast(_c7#7130 as int) AS content_size#7143]\n",
            "Input [5]: [_c0#7123, _c3#7126, _c5#7128, _c6#7129, _c7#7130]\n",
            "\n",
            "|\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "root\n",
            " |-- plan: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xBrUNHSBtgv"
      },
      "source": [
        "## What About Datasets?\n",
        "\n",
        "Added in Spark 1.6, a **Dataset** is a distributed collection of data that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (`map`, `flatMap`, `filter`, etc.). The Dataset API is available in Scala and Java. Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally row.columnName). The case for R is similar.\n",
        "\n",
        "A **DataFrame** is a Dataset organized into named columns. ([source](https://spark.apache.org/docs/3.1.1/sql-programming-guide.html)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ2zn6ezgTmF"
      },
      "source": [
        "# Reporting Tasks (from Lab 1)\n",
        "\n",
        "\n",
        "1. Most popular URL paths (top 15)\n",
        "2. Request count for each HTTP response code, sorted by response code\n",
        "3. Request count for each calendar month and year, sorted chronologically\n",
        "4. Total bytes sent to the client with a specified hostname or IPv4 address (you may hard code an address)\n",
        "5. Based on a given URL (hard coded), compute a request count for each client (hostname or IPv4) who accessed that URL, sorted by request count, highest to lowest\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlPvNuJGgkQl"
      },
      "source": [
        "# (A) RDD Implementations\n",
        "\n",
        "Perform reporting tasks 1-5 using RDD transformations\n",
        "\n",
        "[RDD APIs PySpark v3.1.1](https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.html#rdd-apis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzNTpq69g1pU",
        "outputId": "4e071db1-63e4-472b-8780-b252c5f81565"
      },
      "source": [
        "# RDD implementation\n",
        "# (1) Most popular URL paths (top 15)\n",
        "\n",
        "# find the top 10 clients, map/reduce style using RDD transformations\n",
        "access_log_rdd = (sc.textFile(\"apache.access.log\")\n",
        "                  .map(lambda line: ( line.split(\" \")[6], 1 ))  # field 0 = client address\n",
        "                  .reduceByKey(lambda x, y: x + y)\n",
        "                  .sortBy(lambda t: -t[1])) \n",
        "\n",
        "print(access_log_rdd.take(15))\n"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('/twiki/bin/view/Main/WebHome', 40), ('/twiki/pub/TWiki/TWikiLogos/twikiRobot46x50.gif', 32), ('/', 31), ('/favicon.ico', 28), ('/robots.txt', 27), ('/razor.html', 23), ('/twiki/bin/view/Main/SpamAssassinTaggingOnly', 18), ('/twiki/bin/view/Main/SpamAssassinAndPostFix', 17), ('/cgi-bin/mailgraph2.cgi', 16), ('/cgi-bin/mailgraph.cgi/mailgraph_0.png', 16), ('/cgi-bin/mailgraph.cgi/mailgraph_1_err.png', 16), ('/cgi-bin/mailgraph.cgi/mailgraph_1.png', 16), ('/cgi-bin/mailgraph.cgi/mailgraph_0_err.png', 16), ('/cgi-bin/mailgraph.cgi/mailgraph_2.png', 16), ('/cgi-bin/mailgraph.cgi/mailgraph_2_err.png', 16)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az250Dgghvd5",
        "outputId": "294413fa-7c7f-4285-cad4-21b41b1b9943"
      },
      "source": [
        "# RDD implementation\n",
        "# (2) Request count for each HTTP response code, sorted by response code\n",
        "\n",
        "access_log_rdd = (sc.textFile(\"apache.access.log\")\n",
        "                  .map(lambda line: ( line.split(\" \")[8], 1 ))  # field 0 = client address\n",
        "                  .reduceByKey(lambda x, y: x + y)\n",
        "                  .sortBy(lambda t: t[0])) \n",
        "\n",
        "print(access_log_rdd.take(access_log_rdd.count()))\n"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('200', 1272), ('302', 6), ('401', 123), ('404', 5)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa17VZSyhv4X",
        "outputId": "32c14377-3f95-43c2-b6f0-343de2ca1418"
      },
      "source": [
        "# RDD implementation\n",
        "# (3) Request count for each calendar month and year, sorted chronologically\n",
        "\n",
        "monthsToInt = {\"Jan\": 1, \"Feb\": 2, \"Mar\": 3, \"Apr\": 4, \"May\": 5, \"Jun\":6, \"Jul\": 7, \"Aug\":8, \"Sep\": 9, \"Oct\": 10, \"Nov\":11, \"Dec\":12}\n",
        "intToMonth = {1:\"Jan\", 2:\"Feb\", 3:\"Mar\", 4:\"Apr\", 5:\"May\", 6:\"Jun\", 7:\"Jul\", 8:\"Aug\", 9:\"Sep\", 10:\"Oct\", 11:\"Nov\", 12:\"Dec\"}\n",
        "\n",
        "def makeKey(line):\n",
        "  tokens = line.split(\" \")\n",
        "  month = tokens[3][4:7]\n",
        "  year = tokens[3][8:12]\n",
        "  return year + str(monthsToInt[month])\n",
        "\n",
        "def unpackKey(key):\n",
        "  year = key[:4]\n",
        "  month = key[4:]\n",
        "  return intToMonth[int(month)] + \"/\" + year\n",
        "\n",
        "access_log_rdd = (sc.textFile(\"apache.access.log\")\n",
        "                  .map(lambda line: (makeKey(line), 1))\n",
        "                  .reduceByKey(lambda x, y: x + y)\n",
        "                  .sortBy(lambda t: t[0])\n",
        "                  .map(lambda pair : (unpackKey(pair[0]), pair[1])))\n",
        "\n",
        "print(access_log_rdd.take(access_log_rdd.count()))"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Dec/1999', 1), ('Feb/2003', 1), ('Jan/2004', 1), ('Mar/2004', 1402), ('Jun/2016', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSUdQb4Mhv-W",
        "outputId": "6d6dc438-ab13-4cff-9b28-cb6481c24140"
      },
      "source": [
        "# RDD implementation\n",
        "# (4) Total bytes sent to the client with a specified hostname or IPv4 address (you may hard code an address)\n",
        "\n",
        "HOSTNAME = \"dsl-80-43-113-44.access.uk.tiscali.com\"\n",
        "def filterIP(ip):\n",
        "  return ip == HOSTNAME\n",
        "\n",
        "access_log_rdd = (sc.textFile(\"apache.access.log\")\n",
        "                  .map(lambda line: ( line.split(\" \")[0], int(line.split(\" \")[9])))\n",
        "                  .filter(lambda pair: (filterIP(pair[0])))\n",
        "                  .reduceByKey(lambda x, y: x + y))\n",
        "\n",
        "print(access_log_rdd.take(access_log_rdd.count()))"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('dsl-80-43-113-44.access.uk.tiscali.com', 7125)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi89G8EQhwEO",
        "outputId": "f31f5df4-b0cb-4e80-913e-b80fd4f62294"
      },
      "source": [
        "# RDD implementation\n",
        "# (5) Based on a given URL (hard coded), compute a request count for each client (hostname or IPv4) who accessed that URL, sorted by request count, highest to lowest\n",
        "\n",
        "URL = \"/cgi-bin/mailgraph.cgi/mailgraph_0.png\"\n",
        "def filterUrl(url):\n",
        "  return url == URL\n",
        "\n",
        "access_log_rdd = (sc.textFile(\"apache.access.log\")\n",
        "                  .filter(lambda line: (filterUrl(line.split(\" \")[6])))\n",
        "                  .map(lambda line: ( line.split(\" \")[0], 1))\n",
        "                  .reduceByKey(lambda x, y: x + y)\n",
        "                  .sortBy(lambda t: -t[1]))\n",
        "\n",
        "print(access_log_rdd.take(access_log_rdd.count()))"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('10.0.0.153', 12), ('h24-70-69-74.ca.shawcable.net', 2), ('ts04-ip92.hevanet.com', 1), ('3_343_lt_someone', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxkjTvxIhBO-"
      },
      "source": [
        "# (B) DataFrame Implementations\n",
        "\n",
        "Perform reporting tasks 1-5 using Spark's DataFrame API\n",
        "\n",
        "[DataFrame API PySpark v.3.1.1](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhGYTdWLhJCH",
        "outputId": "a1406dcf-0fe0-40d3-efa5-337cdd87def2"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (1) Most popular URL paths (top 15)\n",
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "named_df = access_log_df.select(col('_c0').alias('host'),\n",
        "                                col('_c3').alias('timestamp'),\n",
        "                                col('_c5').alias('path'),\n",
        "                                col('_c6').cast('integer').alias('status'),\n",
        "                                col('_c7').cast('integer').alias('content_size'))\n",
        "top15url = named_df.groupBy('path').count().orderBy('count', ascending=[0]).head(15)\n",
        "print(top15url)"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Row(path='GET /twiki/bin/view/Main/WebHome HTTP/1.1', count=31), Row(path='GET / HTTP/1.1', count=26), Row(path='GET /robots.txt HTTP/1.0', count=25), Row(path='GET /twiki/pub/TWiki/TWikiLogos/twikiRobot46x50.gif HTTP/1.1', count=25), Row(path='GET /favicon.ico HTTP/1.1', count=24), Row(path='GET /twiki/bin/view/Main/SpamAssassinTaggingOnly HTTP/1.1', count=16), Row(path='GET /cgi-bin/mailgraph.cgi/mailgraph_0.png HTTP/1.1', count=16), Row(path='GET /cgi-bin/mailgraph.cgi/mailgraph_2.png HTTP/1.1', count=16), Row(path='GET /razor.html HTTP/1.1', count=16), Row(path='GET /cgi-bin/mailgraph.cgi/mailgraph_0_err.png HTTP/1.1', count=16), Row(path='GET /cgi-bin/mailgraph.cgi/mailgraph_1.png HTTP/1.1', count=16), Row(path='GET /cgi-bin/mailgraph.cgi/mailgraph_1_err.png HTTP/1.1', count=16), Row(path='GET /cgi-bin/mailgraph.cgi/mailgraph_3_err.png HTTP/1.1', count=16), Row(path='GET /cgi-bin/mailgraph2.cgi HTTP/1.1', count=16), Row(path='GET /cgi-bin/mailgraph.cgi/mailgraph_3.png HTTP/1.1', count=16)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apq4hUSVh94J",
        "outputId": "ca78042f-23af-48c4-f987-b94da93b9043"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (2) Request count for each HTTP response code, sorted by response code\n",
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "named_df = access_log_df.select(col('_c0').alias('host'),\n",
        "                                col('_c3').alias('timestamp'),\n",
        "                                col('_c5').alias('path'),\n",
        "                                col('_c6').cast('integer').alias('status'),\n",
        "                                col('_c7').cast('integer').alias('content_size'))\n",
        "httpResponseCounts = named_df.groupBy('status').count().orderBy('status').collect()\n",
        "print(httpResponseCounts)"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Row(status=200, count=1272), Row(status=302, count=6), Row(status=401, count=123), Row(status=404, count=5)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSvjvCNth9-S",
        "outputId": "d3a7d598-3adc-47fb-b389-07c4767de6ea"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (3) Request count for each calendar month and year, sorted chronologically\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "monthsToInt = {\"Jan\": 1, \"Feb\": 2, \"Mar\": 3, \"Apr\": 4, \"May\": 5, \"Jun\":6, \"Jul\": 7, \"Aug\":8, \"Sep\": 9, \"Oct\": 10, \"Nov\":11, \"Dec\":12}\n",
        "intToMonth = {1:\"Jan\", 2:\"Feb\", 3:\"Mar\", 4:\"Apr\", 5:\"May\", 6:\"Jun\", 7:\"Jul\", 8:\"Aug\", 9:\"Sep\", 10:\"Oct\", 11:\"Nov\", 12:\"Dec\"}\n",
        "\n",
        "def makeKey(line):\n",
        "  month = line[4:7]\n",
        "  year = line[8:12]\n",
        "  return year + str(monthsToInt[month])\n",
        "\n",
        "def unpackKey(key):\n",
        "  year = key[:4]\n",
        "  month = key[4:]\n",
        "  return intToMonth[int(month)] + \"/\" + year\n",
        "\n",
        "udf_makeKey = udf(makeKey, StringType())\n",
        "udf_unpackKey = udf(unpackKey, StringType())\n",
        "\n",
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "named_df = access_log_df.select(col('_c0').alias('host'),\n",
        "                                col('_c3').alias('timestamp'),\n",
        "                                col('_c5').alias('path'),\n",
        "                                col('_c6').cast('integer').alias('status'),\n",
        "                                col('_c7').cast('integer').alias('content_size'))\n",
        "\n",
        "dateRequestCountHashed = named_df.withColumn('timeHash', udf_makeKey(named_df.timestamp)).select('timehash').orderBy('timeHash').groupBy('timeHash').count()\n",
        "dateRequestCount = dateRequestCountHashed.withColumn('Date', udf_unpackKey(dateRequestCountHashed.timeHash)).select('Date', 'count')\n",
        "dateRequestCount.show(truncate=False)\n",
        "dateRequestCount.printSchema()"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-----+\n",
            "|Date    |count|\n",
            "+--------+-----+\n",
            "|Dec/1999|1    |\n",
            "|Feb/2003|1    |\n",
            "|Jan/2004|1    |\n",
            "|Mar/2004|1402 |\n",
            "|Jun/2016|1    |\n",
            "+--------+-----+\n",
            "\n",
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- count: long (nullable = false)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjkoHWOxh-Df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dea8150-6cb0-4763-eb4e-4d00ff57265a"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (4) Total bytes sent to the client with a specified hostname or IPv4 address (you may hard code an address)\n",
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "named_df = access_log_df.select(col('_c0').alias('host'),\n",
        "                                col('_c3').alias('timestamp'),\n",
        "                                col('_c5').alias('path'),\n",
        "                                col('_c6').cast('integer').alias('status'),\n",
        "                                col('_c7').cast('integer').alias('content_size'))\n",
        "HOSTNAME = \"dsl-80-43-113-44.access.uk.tiscali.com\"\n",
        "httpResponseCounts = named_df.filter(named_df['host'] == HOSTNAME).select('content_size').groupBy().sum().collect()\n",
        "print(httpResponseCounts)"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Row(sum(content_size)=7125)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiwpDYxTh-Iv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ae9bf0-5ba2-4d74-c7ea-15f67e5cb2eb"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (5) Based on a given URL (hard coded), compute a request count for each client (hostname or IPv4) who accessed that URL, sorted by request count, highest to lowest\n",
        "\n",
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "named_df = access_log_df.select(col('_c0').alias('host'),\n",
        "                                col('_c3').alias('timestamp'),\n",
        "                                col('_c5').alias('path'),\n",
        "                                col('_c6').cast('integer').alias('status'),\n",
        "                                col('_c7').cast('integer').alias('content_size'))\n",
        "URL = \"/cgi-bin/mailgraph.cgi/mailgraph_0.png\"\n",
        "clientRequestCounts = named_df.filter(named_df['path'].contains(URL)).groupBy('host').count().orderBy('count', ascending=[0]).collect()\n",
        "print(clientRequestCounts)"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Row(host='10.0.0.153', count=12), Row(host='h24-70-69-74.ca.shawcable.net', count=2), Row(host='3_343_lt_someone', count=1), Row(host='ts04-ip92.hevanet.com', count=1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ml-MwXXhbT6"
      },
      "source": [
        "# (C) Spark SQL Implementations\n",
        "\n",
        "Perform reporting tasks 1-5 using Spark SQL\n",
        "\n",
        "[Spark SQL API PySpark v3.1.1](https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html)\n",
        "\n",
        "Specifically, [pyspark.sql.SparkSession.sql](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.sql.html#pyspark.sql.SparkSession.sql) returns a DataFrame representing the result of the given SQL query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kk-wMyahiZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e2c01a-0b9a-4c00-8c15-7bcb4568bf23"
      },
      "source": [
        "# Spark SQL implementation \n",
        "# (1) Most popular URL paths (top 15)\n",
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "named_df = access_log_df.select(col('_c0').alias('host'),\n",
        "                                col('_c3').alias('timestamp'),\n",
        "                                col('_c5').alias('path'),\n",
        "                                col('_c6').cast('integer').alias('status'),\n",
        "                                col('_c7').cast('integer').alias('content_size'))\n",
        "named_df.createOrReplaceTempView(\"log\")\n",
        "sql_df = spark.sql(\"SELECT path AS path, count(path) FROM log GROUP BY path ORDER BY count(path) DESC LIMIT 15;\")\n",
        "\n",
        "\n",
        "sql_df.show(truncate=False)\n",
        "sql_df.printSchema()"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------+-----------+\n",
            "|path                                                        |count(path)|\n",
            "+------------------------------------------------------------+-----------+\n",
            "|GET /twiki/bin/view/Main/WebHome HTTP/1.1                   |31         |\n",
            "|GET / HTTP/1.1                                              |26         |\n",
            "|GET /twiki/pub/TWiki/TWikiLogos/twikiRobot46x50.gif HTTP/1.1|25         |\n",
            "|GET /robots.txt HTTP/1.0                                    |25         |\n",
            "|GET /favicon.ico HTTP/1.1                                   |24         |\n",
            "|GET /cgi-bin/mailgraph2.cgi HTTP/1.1                        |16         |\n",
            "|GET /cgi-bin/mailgraph.cgi/mailgraph_1.png HTTP/1.1         |16         |\n",
            "|GET /razor.html HTTP/1.1                                    |16         |\n",
            "|GET /cgi-bin/mailgraph.cgi/mailgraph_0_err.png HTTP/1.1     |16         |\n",
            "|GET /cgi-bin/mailgraph.cgi/mailgraph_3_err.png HTTP/1.1     |16         |\n",
            "|GET /cgi-bin/mailgraph.cgi/mailgraph_1_err.png HTTP/1.1     |16         |\n",
            "|GET /cgi-bin/mailgraph.cgi/mailgraph_2.png HTTP/1.1         |16         |\n",
            "|GET /cgi-bin/mailgraph.cgi/mailgraph_0.png HTTP/1.1         |16         |\n",
            "|GET /twiki/bin/view/Main/SpamAssassinTaggingOnly HTTP/1.1   |16         |\n",
            "|GET /cgi-bin/mailgraph.cgi/mailgraph_3.png HTTP/1.1         |16         |\n",
            "+------------------------------------------------------------+-----------+\n",
            "\n",
            "root\n",
            " |-- path: string (nullable = true)\n",
            " |-- count(path): long (nullable = false)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEHU_koHiFLH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebf30345-64f1-4127-a23d-e0c67151d723"
      },
      "source": [
        "# Spark SQL implementation \n",
        "# (2) Request count for each HTTP response code, sorted by response code\n",
        "\n",
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "named_df = access_log_df.select(col('_c0').alias('host'),\n",
        "                                col('_c3').alias('timestamp'),\n",
        "                                col('_c5').alias('path'),\n",
        "                                col('_c6').cast('integer').alias('status'),\n",
        "                                col('_c7').cast('integer').alias('content_size'))\n",
        "named_df.createOrReplaceTempView(\"log\")\n",
        "sql_df = spark.sql(\"SELECT status AS status, count(status) FROM log GROUP BY status ORDER BY status;\")\n",
        "sql_df.show(truncate=False)\n",
        "sql_df.printSchema()"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-------------+\n",
            "|status|count(status)|\n",
            "+------+-------------+\n",
            "|200   |1272         |\n",
            "|302   |6            |\n",
            "|401   |123          |\n",
            "|404   |5            |\n",
            "+------+-------------+\n",
            "\n",
            "root\n",
            " |-- status: integer (nullable = true)\n",
            " |-- count(status): long (nullable = false)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_DVj46HiFQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e30c5e50-b63b-4c40-9c8d-40c37e2aacc1"
      },
      "source": [
        "# Spark SQL implementation \n",
        "# (3) Request count for each calendar month and year, sorted chronologically\n",
        "\n",
        "monthsToInt = {\"Jan\": 1, \"Feb\": 2, \"Mar\": 3, \"Apr\": 4, \"May\": 5, \"Jun\":6, \"Jul\": 7, \"Aug\":8, \"Sep\": 9, \"Oct\": 10, \"Nov\":11, \"Dec\":12}\n",
        "intToMonth = {1:\"Jan\", 2:\"Feb\", 3:\"Mar\", 4:\"Apr\", 5:\"May\", 6:\"Jun\", 7:\"Jul\", 8:\"Aug\", 9:\"Sep\", 10:\"Oct\", 11:\"Nov\", 12:\"Dec\"}\n",
        "\n",
        "def makeKey(line):\n",
        "  month = line[4:7]\n",
        "  year = line[8:12]\n",
        "  return year + str(monthsToInt[month])\n",
        "\n",
        "def unpackKey(key):\n",
        "  year = key[:4]\n",
        "  month = key[4:]\n",
        "  return intToMonth[int(month)] + \"/\" + year\n",
        "\n",
        "udf_makeKey = udf(makeKey, StringType())\n",
        "udf_unpackKey = udf(unpackKey, StringType())\n",
        "\n",
        "spark.udf.register(\"makeKey\", makeKey)\n",
        "spark.udf.register(\"unpackKey\", unpackKey)\n",
        "\n",
        "\n",
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "named_df = access_log_df.select(col('_c0').alias('host'),\n",
        "                                col('_c3').alias('timestamp'),\n",
        "                                col('_c5').alias('path'),\n",
        "                                col('_c6').cast('integer').alias('status'),\n",
        "                                col('_c7').cast('integer').alias('content_size'))\n",
        "named_df.createOrReplaceTempView(\"log\")\n",
        "sql_df1 = spark.sql(\"SELECT timestamp, makeKey(timestamp) as hash FROM log group by hash, timestamp;\")\n",
        "sql_df1.createOrReplaceTempView(\"table1\")\n",
        "sql_df = spark.sql(\"SELECT hash, unpackKey(hash) as date, count(hash) as count FROM table1 group by date, hash order by hash\")\n",
        "sql_df.createOrReplaceTempView(\"table2\")\n",
        "dateRequest = spark.sql(\"SELECT date, count from table2\")\n",
        "dateRequest.show(truncate=False)\n",
        "dateRequest.printSchema()"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-----+\n",
            "|date    |count|\n",
            "+--------+-----+\n",
            "|Dec/1999|1    |\n",
            "|Feb/2003|1    |\n",
            "|Jan/2004|1    |\n",
            "|Mar/2004|1087 |\n",
            "|Jun/2016|1    |\n",
            "+--------+-----+\n",
            "\n",
            "root\n",
            " |-- date: string (nullable = true)\n",
            " |-- count: long (nullable = false)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOvUdlziFUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb506041-3cd4-40c3-dfd7-b4d3601d7e6c"
      },
      "source": [
        "# Spark SQL implementation \n",
        "# (4) Total bytes sent to the client with a specified hostname or IPv4 address (you may hard code an address)\n",
        "\n",
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "named_df = access_log_df.select(col('_c0').alias('host'),\n",
        "                                col('_c3').alias('timestamp'),\n",
        "                                col('_c5').alias('path'),\n",
        "                                col('_c6').cast('integer').alias('status'),\n",
        "                                col('_c7').cast('integer').alias('content_size'))\n",
        "named_df.createOrReplaceTempView(\"log\")\n",
        "sql_df = spark.sql(\"SELECT host AS host, SUM(content_size) FROM log WHERE host='dsl-80-43-113-44.access.uk.tiscali.com' GROUP BY host;\")\n",
        "sql_df.show(truncate=False)\n",
        "sql_df.printSchema()"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------+-----------------+\n",
            "|host                                  |sum(content_size)|\n",
            "+--------------------------------------+-----------------+\n",
            "|dsl-80-43-113-44.access.uk.tiscali.com|7125             |\n",
            "+--------------------------------------+-----------------+\n",
            "\n",
            "root\n",
            " |-- host: string (nullable = true)\n",
            " |-- sum(content_size): long (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYxy0gSoiFYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "151ef5b1-943e-4e9c-fb6e-b9ddd4685d14"
      },
      "source": [
        "# Spark SQL implementation \n",
        "# (5) Based on a given URL (hard coded), compute a request count for each client (hostname or IPv4) who accessed that URL, sorted by request count, highest to lowest\n",
        "\n",
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "named_df = access_log_df.select(col('_c0').alias('host'),\n",
        "                                col('_c3').alias('timestamp'),\n",
        "                                col('_c5').alias('path'),\n",
        "                                col('_c6').cast('integer').alias('status'),\n",
        "                                col('_c7').cast('integer').alias('content_size'))\n",
        "named_df.createOrReplaceTempView(\"log\")\n",
        "\n",
        "sql_df = spark.sql(\"SELECT path, host, count(host) FROM log WHERE path = 'GET /cgi-bin/mailgraph.cgi/mailgraph_0.png HTTP/1.1' group by host, path order by count(host) desc;\")\n",
        "sql_df.show(truncate=False)\n",
        "sql_df.printSchema()"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------+-----------------------------+-----------+\n",
            "|path                                               |host                         |count(host)|\n",
            "+---------------------------------------------------+-----------------------------+-----------+\n",
            "|GET /cgi-bin/mailgraph.cgi/mailgraph_0.png HTTP/1.1|10.0.0.153                   |12         |\n",
            "|GET /cgi-bin/mailgraph.cgi/mailgraph_0.png HTTP/1.1|h24-70-69-74.ca.shawcable.net|2          |\n",
            "|GET /cgi-bin/mailgraph.cgi/mailgraph_0.png HTTP/1.1|3_343_lt_someone             |1          |\n",
            "|GET /cgi-bin/mailgraph.cgi/mailgraph_0.png HTTP/1.1|ts04-ip92.hevanet.com        |1          |\n",
            "+---------------------------------------------------+-----------------------------+-----------+\n",
            "\n",
            "root\n",
            " |-- path: string (nullable = true)\n",
            " |-- host: string (nullable = true)\n",
            " |-- count(host): long (nullable = false)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}